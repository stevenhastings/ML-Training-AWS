{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPGgKCvnIzNNW/uLqLzG5+e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stevenhastings/ML-Training-AWS/blob/main/machineLearningDevelopment%26Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***DATA COLLECTION AND DATA MUNGING***"
      ],
      "metadata": {
        "id": "qbz9lp3iSjSE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Sources\n",
        "## Relational Databases or SQL Databases\n",
        "* Transaction Processing\n",
        "* Data Warehousing\n",
        "## NoSQL Databases\n",
        "* Support Web and some types of analytic applications\n",
        "## Spreadsheets\n",
        "* Smaller Semi-informal sources of data\n",
        "* Combine small datasets and make specialized calculations\n",
        "* Difficult due to frequently changing structures\n",
        "## Log files\n",
        "* Generated by applications and devices\n",
        "* Tend to be semi-structured\n",
        "* But tools like Microsofts log-parser are useful for mapping to more structured formats.\n",
        "## External data sources\n",
        "* 3rd party data files\n",
        "* APIs that are programmatically queried"
      ],
      "metadata": {
        "id": "gaTeaUJXRCVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Transform Load (ETL) or Filter Extract Reformat\n",
        "* Once data is in hand it must be analyzed to understand how the data needs to be filtered and reformatted to meet the needs of the data modeling requirements. \n",
        "* Within a single data source:\n",
        " * identify relevant attributes\n",
        " * filter unnecessary content\n",
        " * reformat to modeling needs\n",
        "\n",
        "#### Combining Data Sets\n",
        "* Join on common attributes\n",
        "* consolidate attributes\n",
        "* build tabular data structure (DataFrame)"
      ],
      "metadata": {
        "id": "c21VsqjESpjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimenting with data, features, and algorithms\n",
        "* Feature Engineering:\n",
        " * Question: What additional features can be derived from original attributes?\n",
        " * Helpful for some algorithms such as *Decision Trees* but less helpful for things like Neural Networks since those algorithms can capture non-linear relationships within the data. \n",
        "\n",
        "* Algorithm Selection:\n",
        " * Evaluating quality of models built using a variety of algorithms. \n",
        " * Ensemble Method: Combining the results of many different Machine Learning models to create the model of best fit for your use case. \n"
      ],
      "metadata": {
        "id": "ywOHV9ODTf3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing and Validating Models\n",
        "* ***Training Data:***\n",
        " * used to build a model with data that is available at some point in time.\n",
        "* ***Test Data*** is used in development to validate the models success on new and unseen data.\n",
        " * Used to evaluate the model before deploying the model into production.  \n",
        "* ***Validation data*** is used to measure quality of predictions made in production (AFTER MODEL DEPLOYMENT). . .to avoid model drift.\n"
      ],
      "metadata": {
        "id": "e-PRSEkxVFWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Data Science Models\n",
        "1. Build Model\n",
        "2. Evaluate Model\n",
        "3. Implement Changes to model\n",
        "4. Try again"
      ],
      "metadata": {
        "id": "I-MloAwXWCHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version Control:\n",
        "* Tracking each version of a program\n",
        " * GitHub\n",
        " > - Repositories\n",
        " > - Adding files\n",
        " > - Committing files\n",
        " > - Cloning Repositories"
      ],
      "metadata": {
        "id": "e_RTjkSZWSNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictive Model Markup Language (PMML)\n",
        "* XML standard \n",
        "* a machine learning interchange format for describing predictive models\n",
        "* ELEMENTS OF PMML:\n",
        " * Data dictionary\n",
        " > - < DataDictionary numberOfFields=\"5\" >\n",
        " > - < DataField dataType=\"double\" name=\"sepal_length\" optype=\"continuous\" >\n",
        " > - < Interval closure=\"closedClosed\" leftMargin=\"4.3\" rightMargin=\"7.9\">\n",
        "\n",
        " * Transformations\n",
        " * Models\n",
        " * Post-processing\n",
        "\n",
        "BENEFITS:\n",
        "\n",
        "* Building in exploratory tools\n",
        "* Deploying in production platforms\n",
        "* Standard description of models\n",
        "* Access to coefficients"
      ],
      "metadata": {
        "id": "8VHcWBLKW6dW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agile Development\n",
        "* Continuous Integration (CI)\n",
        " * Frequent Deployment\n",
        " * Build and implement an application with small changes\n",
        " * Detect problems and make changes SOONER rather than LATER\n",
        "\n",
        "* CI for Data Science\n",
        " * Build model\n",
        " * Check in code\n",
        " * Build deployment package\n",
        " * Deploy\n",
        "\n",
        "* Jenkins\n",
        " * A tool for automated continuous deployment\n",
        " * Open Sourced tool \n",
        " * written in Java\n",
        " * Integrates well with version control systems like Git\n",
        "\n",
        "* Jenkins Pipeline\n",
        " * Delivery pipeline in code\n",
        " * supports multiple steps\n",
        " * defines execution environments\n",
        " * records test results\n",
        " * deploys"
      ],
      "metadata": {
        "id": "Zv6goQNVYeaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environments\n",
        "* Router\n",
        "* Load Balancer\n",
        "* Application Server\n",
        "* Cache\n",
        "* Database\n",
        "\n",
        "> Types of Environments:\n",
        " 1. Staging:\n",
        "  * integration testing\n",
        " 2. Development\n",
        "  * Canary Deployment:\n",
        "  > - a roll out that changes the code only for some users when using multiple application servers we can deploy a new roll out to just one of the servers now only the users working on that server will be running the new model. We can carefully monitor the error rate and other metrics on that server to determine whether there are problems with the release. If you use load balancers to distribute your work load across servers you can configure your load balancer to distribute traffic to each server. For example you can direct only 10% of traffic to the server running the Canary deployment. This allows you to test your new model without exposing all of your users to a potentially flawed release. \n",
        " 3. Production"
      ],
      "metadata": {
        "id": "7MxGono9ZTJg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Security Measures (securing the data science models in production)\n",
        "* ***Access Controls***\n",
        " 1. *Authentication*\n",
        " > - Confirming the identity of a person, process, or device.\n",
        " 2. *Authorization*\n",
        " > - Rules or set of actions designed to declare WHO is allowed to do WHAT within an application. (make use of existing AUTH systems)\n",
        "* ***Software Development security***\n",
        " * Identifying risks and vulnerabilities\n",
        " * Encryption\n",
        " * Testing security measures\n",
        " * data management\n",
        "* ***Operations Security***\n",
        " * separation of duties\n",
        " * change management ( who can make changes )\n",
        " * Log monitoring\n",
        " * Audit Reviews\n",
        "* ***Disaster Recovery***\n",
        " 1. Backups\n",
        " 2. Design for high availability (Load Balancers)\n",
        " 3. Game Day Failures (Fail Fast) ( Try to break the system)\n",
        " 4. Business Continuity Planning"
      ],
      "metadata": {
        "id": "JIm04I2yb13d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance Monitoring\n",
        "* Resource Utilization\n",
        " * Monitoring the use of CPU, GPU, persistent storage, network resources, etc. (making sure all are sufficient for your needs)\n",
        "* Service Availability\n",
        " * Verify core processes are running\n",
        " * Are API endpoints accessible?\n",
        " * ARe APIs returning non-errors?\n",
        " * Are API calls timing out? \n",
        "* Throughput\n",
        " * Volume of transactions\n",
        " * Backlog of requests\n",
        " * Time to deliver results\n",
        "* Model quality\n",
        " * Sample predictions\n",
        " * evaluate accuracy\n",
        " * watch for model drift\n",
        " * identify new training instances"
      ],
      "metadata": {
        "id": "LBDoGiCTft03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Containers\n",
        "* Docker Components\n",
        " * Docker daemon\n",
        " * Docker client\n",
        " * Docker images\n",
        " * Docker Registries\n",
        " > - repositories that manage the storage of docker images\n",
        "\n",
        "* Docker Architecture\n",
        " * Client:\n",
        " > - Command-line application used for interacting with Docker daemon\n",
        " * Host:\n",
        " > - Server running daemon, running containers, which are executing instances of images\n",
        " * Registry:\n",
        " > - Store of docker images; may be public or private\n",
        " > - The DOCKER HUB is a widely used Docker Repository"
      ],
      "metadata": {
        "id": "_XxCa4xngjYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More Docker\n",
        "## ***Dockerfile***\n",
        "* A text file with configuration image including base image, packages, network ports, and startup commands\n",
        "\n",
        "## ***Docker Commands***\n",
        "* FROM \n",
        " * pulls a base image from your docker repository\n",
        " \n",
        " EXAMPLE:\n",
        "* Base Image\n",
        " * ***FROM*** python:3.6\n",
        "\n",
        "Update OS Packages...\n",
        "* ***RUN*** apt-get update\n",
        "\n",
        "Copy Local Files to Container\n",
        "* ***COPY*** requirements.txt requirements.txt\n",
        "\n",
        "Make Container Accessible\n",
        "* ***EXPOSE*** 8888\n",
        "\n",
        "Execute Script\n",
        "* ***CMD*** python3 data_sci_code.py\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Example Dockerfile:\n",
        "\n",
        "```\n",
        "#Start with the 3.6 version of python\n",
        "FROM python:3.6\n",
        "\n",
        "#Update packages installed in the operating system\n",
        "RUN apt-get update\n",
        "\n",
        "#Copy the list of python packages to install, e.g numpy, scikit-learn\n",
        "COPY requirements.txt requirements.txt\n",
        "\n",
        "#Now install the python packages\n",
        "RUN pip install -r requirements.txt\n",
        "\n",
        "#Allow access on port 8888\n",
        "EXPOSE 8888\n",
        "\n",
        "#When the container starts, run the script called data_sci_code.py\n",
        "CMD python3 data_sci_code.py\n",
        "```\n",
        "\n",
        "#### Build Docker Image:\n",
        "`docker build -t devops_example`\n",
        "#### Run a Docker Image\n",
        "`docker run -t devops_example`\n"
      ],
      "metadata": {
        "id": "Qger5eWchUZF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FRFsm8fqjzbb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}